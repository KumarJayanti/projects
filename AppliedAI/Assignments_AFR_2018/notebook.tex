
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{05 Amazon Fine Food Reviews Analysis\_Logistic Regression}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Amazon Fine Food Reviews
Analysis}\label{amazon-fine-food-reviews-analysis}

Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews

EDA:
https://nycdatascience.com/blog/student-works/amazon-fine-foods-visualization/

The Amazon Fine Food Reviews dataset consists of reviews of fine foods
from Amazon.

Number of reviews: 568,454 Number of users: 256,059 Number of products:
74,258 Timespan: Oct 1999 - Oct 2012 Number of Attributes/Columns in
data: 10

Attribute Information:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Id
\item
  ProductId - unique identifier for the product
\item
  UserId - unqiue identifier for the user
\item
  ProfileName
\item
  HelpfulnessNumerator - number of users who found the review helpful
\item
  HelpfulnessDenominator - number of users who indicated whether they
  found the review helpful or not
\item
  Score - rating between 1 and 5
\item
  Time - timestamp for the review
\item
  Summary - brief summary of the review
\item
  Text - text of the review
\end{enumerate}

\paragraph{Objective:}\label{objective}

Given a review, determine whether the review is positive (rating of 4 or
5) or negative (rating of 1 or 2).

 {[}Q{]} How to determine if a review is positive or negative? {[}Ans{]}
We could use Score/Rating. A rating of 4 or 5 can be cosnidered as a
positive review. A rating of 1 or 2 can be considered as negative one. A
review of rating 3 is considered nuetral and such reviews are ignored
from our analysis. This is an approximate and proxy way of determining
the polarity (positivity/negativity) of a review.

    \section{{[}1{]}. Reading Data}\label{reading-data}

    \subsection{{[}1.1{]} Loading the data}\label{loading-the-data}

The dataset is available in two forms 1. .csv file 2. SQLite Database

In order to load the data, We have used the SQLITE dataset as it is
easier to query the data and visualise the data efficiently.

Here as we only want to get the global sentiment of the recommendations
(positive or negative), we will purposefully ignore all Scores equal to
3. If the score is above 3, then the recommendation wil be set to
"positive". Otherwise, it will be set to "negative".

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{k+kn}{import} \PY{n+nn}{warnings}
        \PY{n}{warnings}\PY{o}{.}\PY{n}{filterwarnings}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        
        \PY{k+kn}{import} \PY{n+nn}{sqlite3}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{nltk}
        \PY{k+kn}{import} \PY{n+nn}{string}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{TfidfTransformer}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{TfidfVectorizer}
        
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{CountVectorizer}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{metrics}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}curve}\PY{p}{,} \PY{n}{auc}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem}\PY{n+nn}{.}\PY{n+nn}{porter} \PY{k}{import} \PY{n}{PorterStemmer}
        
        \PY{k+kn}{import} \PY{n+nn}{re}
        \PY{c+c1}{\PYZsh{} Tutorial about Python regular expressions: https://pymotw.com/2/re/}
        \PY{k+kn}{import} \PY{n+nn}{string}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{stopwords}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem} \PY{k}{import} \PY{n}{PorterStemmer}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem}\PY{n+nn}{.}\PY{n+nn}{wordnet} \PY{k}{import} \PY{n}{WordNetLemmatizer}
        
        \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Word2Vec}
        \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{KeyedVectors}
        \PY{k+kn}{import} \PY{n+nn}{pickle}
        
        \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
        \PY{k+kn}{import} \PY{n+nn}{os}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} using SQLite Table to read data.}
        \PY{n}{con} \PY{o}{=} \PY{n}{sqlite3}\PY{o}{.}\PY{n}{connect}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{database.sqlite}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
        
        \PY{c+c1}{\PYZsh{} filtering only positive and negative reviews i.e. }
        \PY{c+c1}{\PYZsh{} not taking into consideration those reviews with Score=3}
        \PY{c+c1}{\PYZsh{} SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000, will give top 500000 data points}
        \PY{c+c1}{\PYZsh{} you can change the number to any other number based on your computing power}
        
        \PY{c+c1}{\PYZsh{} filtered\PYZus{}data = pd.read\PYZus{}sql\PYZus{}query(\PYZdq{}\PYZdq{}\PYZdq{} SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000\PYZdq{}\PYZdq{}\PYZdq{}, con) }
        \PY{c+c1}{\PYZsh{} for tsne assignment you can take 5k data points}
        
        \PY{c+c1}{\PYZsh{}filtered\PYZus{}data = pd.read\PYZus{}sql\PYZus{}query(\PYZdq{}\PYZdq{}\PYZdq{} SELECT * FROM Reviews WHERE Score != 3 LIMIT 5000\PYZdq{}\PYZdq{}\PYZdq{}, con) }
        
        \PY{c+c1}{\PYZsh{} 100000 data points for Logistic Regression.}
        \PY{n}{filtered\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}sql\PYZus{}query}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{l+s+s2}{ SELECT * FROM Reviews WHERE Score != 3 ORDER BY Time DESC LIMIT 100000}\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{p}{,} \PY{n}{con}\PY{p}{)} 
        
        
        
        
        \PY{c+c1}{\PYZsh{} Give reviews with Score\PYZgt{}3 a positive rating(1), and reviews with a score\PYZlt{}3 a negative rating(0).}
        \PY{k}{def} \PY{n+nf}{partition}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{n}{x} \PY{o}{\PYZlt{}} \PY{l+m+mi}{3}\PY{p}{:}
                \PY{k}{return} \PY{l+m+mi}{0}
            \PY{k}{return} \PY{l+m+mi}{1}
        
        \PY{c+c1}{\PYZsh{}changing reviews with score less than 3 to be positive and vice\PYZhy{}versa}
        \PY{n}{actualScore} \PY{o}{=} \PY{n}{filtered\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{positiveNegative} \PY{o}{=} \PY{n}{actualScore}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{partition}\PY{p}{)} 
        \PY{n}{filtered\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{positiveNegative}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of data points in our data}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{filtered\PYZus{}data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n}{filtered\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of data points in our data (100000, 10)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}      Id   ProductId          UserId    ProfileName  HelpfulnessNumerator  \textbackslash{}
        0    10  B00171APVA  A21BT40VZCCYT4  Carol A. Reed                     0   
        1  1089  B004FD13RW    A1BPLP0BKERV           Paul                     0   
        2  5703  B009WSNWC4   AMP7K1O84DH1T           ESTY                     0   
        
           HelpfulnessDenominator  Score        Time           Summary  \textbackslash{}
        0                       0      1  1351209600  Healthy Dog Food   
        1                       0      1  1351209600    It is awesome.   
        2                       0      1  1351209600         DELICIOUS   
        
                                                        Text  
        0  This is a very healthy dog food. Good for thei{\ldots}  
        1  My partner is very happy with the tea, and is {\ldots}  
        2  Purchased this product at a local store in NY {\ldots}  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{display} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}sql\PYZus{}query}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+s2}{SELECT UserId, ProductId, ProfileName, Time, Score, Text, COUNT(*)}
        \PY{l+s+s2}{FROM Reviews}
        \PY{l+s+s2}{GROUP BY UserId}
        \PY{l+s+s2}{HAVING COUNT(*)\PYZgt{}1}
        \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{p}{,} \PY{n}{con}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{display}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n}{display}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(80668, 7)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}                UserId   ProductId             ProfileName        Time  Score  \textbackslash{}
        0  \#oc-R115TNMSPFT9I7  B007Y59HVM                 Breyton  1331510400      2   
        1  \#oc-R11D9D7SHXIJB9  B005HG9ET0  Louis E. Emory "hoppy"  1342396800      5   
        2  \#oc-R11DNU2NBKQ23Z  B007Y59HVM        Kim Cieszykowski  1348531200      1   
        3  \#oc-R11O5J5ZVQE25C  B005HG9ET0           Penguin Chick  1346889600      5   
        4  \#oc-R12KPBODL2B5ZD  B007OSBE1U   Christopher P. Presta  1348617600      1   
        
                                                        Text  COUNT(*)  
        0  Overall its just OK when considering the price{\ldots}         2  
        1  My wife has recurring extreme muscle spasms, u{\ldots}         3  
        2  This coffee is horrible and unfortunately not {\ldots}         2  
        3  This will be the bottle that you grab from the{\ldots}         3  
        4  I didnt like this coffee. Instead of telling y{\ldots}         2  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{display}\PY{p}{[}\PY{n}{display}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{UserId}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AZY10LLTJ71NX}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}               UserId   ProductId                      ProfileName        Time  \textbackslash{}
        80638  AZY10LLTJ71NX  B006P7E5ZI  undertheshrine "undertheshrine"  1334707200   
        
               Score                                               Text  COUNT(*)  
        80638      5  I was recommended to try green tea extract to {\ldots}         5  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{display}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{COUNT(*)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} 393063
\end{Verbatim}
            
    \section{{[}2{]} Exploratory Data
Analysis}\label{exploratory-data-analysis}

    \subsection{{[}2.1{]} Data Cleaning:
Deduplication}\label{data-cleaning-deduplication}

It is observed (as shown in the table below) that the reviews data had
many duplicate entries. Hence it was necessary to remove duplicates in
order to get unbiased results for the analysis of the data. Following is
an example:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{display}\PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}sql\PYZus{}query}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+s2}{SELECT *}
        \PY{l+s+s2}{FROM Reviews}
        \PY{l+s+s2}{WHERE Score != 3 AND UserId=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AR5J8UI46CURR}\PY{l+s+s2}{\PYZdq{}}
        \PY{l+s+s2}{ORDER BY ProductID}
        \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{p}{,} \PY{n}{con}\PY{p}{)}
        \PY{n}{display}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:}        Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \textbackslash{}
        0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   
        1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   
        2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   
        3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   
        4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   
        
           HelpfulnessDenominator  Score        Time  \textbackslash{}
        0                       2      5  1199577600   
        1                       2      5  1199577600   
        2                       2      5  1199577600   
        3                       2      5  1199577600   
        4                       2      5  1199577600   
        
                                     Summary  \textbackslash{}
        0  LOACKER QUADRATINI VANILLA WAFERS   
        1  LOACKER QUADRATINI VANILLA WAFERS   
        2  LOACKER QUADRATINI VANILLA WAFERS   
        3  LOACKER QUADRATINI VANILLA WAFERS   
        4  LOACKER QUADRATINI VANILLA WAFERS   
        
                                                        Text  
        0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS {\ldots}  
        1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS {\ldots}  
        2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS {\ldots}  
        3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS {\ldots}  
        4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS {\ldots}  
\end{Verbatim}
            
    As it can be seen above that same user has multiple reviews with same
values for HelpfulnessNumerator, HelpfulnessDenominator, Score, Time,
Summary and Text and on doing analysis it was found that
ProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies,
8.82-Ounce Packages (Pack of 8) ProductId=B000HDL1RQ was Loacker
Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so
on

It was inferred after analysis that reviews with same parameters other
than ProductId belonged to the same product just having different
flavour or quantity. Hence in order to reduce redundancy it was decided
to eliminate the rows having same parameters.

The method used for the same was that we first sort the data according
to ProductId and then just keep the first similar product review and
delelte the others. for eg. in the above just the review for
ProductId=B000HDL1RQ remains. This method ensures that there is only one
representative for each product and deduplication without sorting would
lead to possibility of different representatives still existing for the
same product.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}Sorting data according to ProductId in ascending order}
        \PY{n}{sorted\PYZus{}data}\PY{o}{=}\PY{n}{filtered\PYZus{}data}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ProductId}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{quicksort}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{na\PYZus{}position}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{last}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{}Deduplication of entries}
        \PY{n}{final}\PY{o}{=}\PY{n}{sorted\PYZus{}data}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{n}{subset}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{UserId}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ProfileName}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Time}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{keep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n}{final}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:} (71551, 10)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{}Checking to see how much \PYZpc{} of data still remains}
         \PY{p}{(}\PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{size}\PY{o}{*}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{filtered\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{size}\PY{o}{*}\PY{l+m+mf}{1.0}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} 71.551
\end{Verbatim}
            
    Observation:- It was also seen that in two rows given below the value of
HelpfulnessNumerator is greater than HelpfulnessDenominator which is not
practically possible hence these two rows too are removed from
calcualtions

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{display}\PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}sql\PYZus{}query}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+s2}{SELECT *}
         \PY{l+s+s2}{FROM Reviews}
         \PY{l+s+s2}{WHERE Score != 3 AND Id=44737 OR Id=64422}
         \PY{l+s+s2}{ORDER BY ProductID}
         \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{p}{,} \PY{n}{con}\PY{p}{)}
         
         \PY{n}{display}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:}       Id   ProductId          UserId              ProfileName  \textbackslash{}
         0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens "Jeanne"   
         1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   
         
            HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \textbackslash{}
         0                     3                       1      5  1224892800   
         1                     3                       2      4  1212883200   
         
                                                 Summary  \textbackslash{}
         0             Bought This for My Son at College   
         1  Pure cocoa taste with crunchy almonds inside   
         
                                                         Text  
         0  My son loves spaghetti so I didn't hesitate or{\ldots}  
         1  It was almost a 'love at first bite' - the per{\ldots}  
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{final}\PY{o}{=}\PY{n}{final}\PY{p}{[}\PY{n}{final}\PY{o}{.}\PY{n}{HelpfulnessNumerator}\PY{o}{\PYZlt{}}\PY{o}{=}\PY{n}{final}\PY{o}{.}\PY{n}{HelpfulnessDenominator}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{}Before starting the next phase of preprocessing lets see the number of entries left}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{final}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}How many positive and negative reviews are present in our dataset?}
         \PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
(71551, 10)

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} 1    59256
         0    12295
         Name: Score, dtype: int64
\end{Verbatim}
            
    \section{{[}3{]} Preprocessing}\label{preprocessing}

    \subsection{{[}3.1{]}. Preprocessing Review
Text}\label{preprocessing-review-text}

Now that we have finished deduplication our data requires some
preprocessing before we go on further with analysis and making the
prediction model.

Hence in the Preprocessing phase we do the following in the order
below:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Begin by removing the html tags
\item
  Remove any punctuations or limited set of special characters like , or
  . or \# etc.
\item
  Check if the word is made up of english letters and is not
  alpha-numeric
\item
  Check to see if the length of the word is greater than 2 (as it was
  researched that there is no adjective in 2-letters)
\item
  Convert the word to lowercase
\item
  Remove Stopwords
\item
  Finally Snowball Stemming the word (it was obsereved to be better than
  Porter Stemming)
\end{enumerate}

After which we collect the words used to describe positive and negative
reviews

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} printing some random reviews}
         \PY{n}{sent\PYZus{}0} \PY{o}{=} \PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}0}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{n}{sent\PYZus{}1000} \PY{o}{=} \PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}1000}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{n}{sent\PYZus{}1500} \PY{o}{=} \PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{1500}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}1500}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{n}{sent\PYZus{}4900} \PY{o}{=} \PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{4900}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}4900}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A charming, rhyming book that describes the circumstances under which you eat (or don't) chicken soup with rice, month-by-month. This sounds like the kind of thing kids would make up while they're out of recess and sing over and over until they drive the teachers crazy. It's cute and catchy and sounds really childlike but is skillfully written.
==================================================
I have one cup a day and it really decreases my night sweats. I'm in amazement at how much it helps!<br /><br />The taste isn't great, but I don't think it's as bad as some other reviews have said. If you're concerned about the taste, you can use some sage from your spice cabinet and make a cup. It tastes pretty much the same, but it didn't seem cost effective to use the sage you buy in the spice aisle.
==================================================
Strong without being bitter, this is my favorite tea by far.  I was pleasantly surprised recently when i showed up for a production job and the craft services table had P\&G Tips.  Really helps on a cold early morning outside.<br /><br />The shipper did exactly as promised.
==================================================
The Kay's Naturals protein crispy Parmesan  pack of 12 protein chips tasted aweful. I would not recommend them to anyone.
==================================================

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} remove urls from text python: https://stackoverflow.com/a/40823105/4084039}
         \PY{n}{sent\PYZus{}0} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sent\PYZus{}0}\PY{p}{)}
         \PY{n}{sent\PYZus{}1000} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sent\PYZus{}1000}\PY{p}{)}
         \PY{n}{sent\PYZus{}150} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sent\PYZus{}1500}\PY{p}{)}
         \PY{n}{sent\PYZus{}4900} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sent\PYZus{}4900}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A charming, rhyming book that describes the circumstances under which you eat (or don't) chicken soup with rice, month-by-month. This sounds like the kind of thing kids would make up while they're out of recess and sing over and over until they drive the teachers crazy. It's cute and catchy and sounds really childlike but is skillfully written.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} https://stackoverflow.com/questions/16206380/python\PYZhy{}beautifulsoup\PYZhy{}how\PYZhy{}to\PYZhy{}remove\PYZhy{}all\PYZhy{}tags\PYZhy{}from\PYZhy{}an\PYZhy{}element}
         \PY{k+kn}{from} \PY{n+nn}{bs4} \PY{k}{import} \PY{n}{BeautifulSoup}
         
         \PY{n}{soup} \PY{o}{=} \PY{n}{BeautifulSoup}\PY{p}{(}\PY{n}{sent\PYZus{}0}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lxml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{text} \PY{o}{=} \PY{n}{soup}\PY{o}{.}\PY{n}{get\PYZus{}text}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{text}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{n}{soup} \PY{o}{=} \PY{n}{BeautifulSoup}\PY{p}{(}\PY{n}{sent\PYZus{}1000}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lxml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{text} \PY{o}{=} \PY{n}{soup}\PY{o}{.}\PY{n}{get\PYZus{}text}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{text}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{n}{soup} \PY{o}{=} \PY{n}{BeautifulSoup}\PY{p}{(}\PY{n}{sent\PYZus{}1500}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lxml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{text} \PY{o}{=} \PY{n}{soup}\PY{o}{.}\PY{n}{get\PYZus{}text}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{text}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
         
         \PY{n}{soup} \PY{o}{=} \PY{n}{BeautifulSoup}\PY{p}{(}\PY{n}{sent\PYZus{}4900}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lxml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{text} \PY{o}{=} \PY{n}{soup}\PY{o}{.}\PY{n}{get\PYZus{}text}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{text}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A charming, rhyming book that describes the circumstances under which you eat (or don't) chicken soup with rice, month-by-month. This sounds like the kind of thing kids would make up while they're out of recess and sing over and over until they drive the teachers crazy. It's cute and catchy and sounds really childlike but is skillfully written.
==================================================
I have one cup a day and it really decreases my night sweats. I'm in amazement at how much it helps!The taste isn't great, but I don't think it's as bad as some other reviews have said. If you're concerned about the taste, you can use some sage from your spice cabinet and make a cup. It tastes pretty much the same, but it didn't seem cost effective to use the sage you buy in the spice aisle.
==================================================
Strong without being bitter, this is my favorite tea by far.  I was pleasantly surprised recently when i showed up for a production job and the craft services table had P\&G Tips.  Really helps on a cold early morning outside.The shipper did exactly as promised.
==================================================
The Kay's Naturals protein crispy Parmesan  pack of 12 protein chips tasted aweful. I would not recommend them to anyone.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} https://stackoverflow.com/a/47091490/4084039}
         \PY{k+kn}{import} \PY{n+nn}{re}
         
         \PY{k}{def} \PY{n+nf}{decontracted}\PY{p}{(}\PY{n}{phrase}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} specific}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{won}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{will not}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{can}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{can not}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} general}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ not}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{re}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ are}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ is}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ would}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ll}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ will}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ not}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ have}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{n}{phrase} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{m}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ am}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{phrase}\PY{p}{)}
             \PY{k}{return} \PY{n}{phrase}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{sent\PYZus{}1500} \PY{o}{=} \PY{n}{decontracted}\PY{p}{(}\PY{n}{sent\PYZus{}1500}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}1500}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Strong without being bitter, this is my favorite tea by far.  I was pleasantly surprised recently when i showed up for a production job and the craft services table had P\&G Tips.  Really helps on a cold early morning outside.<br /><br />The shipper did exactly as promised.
==================================================

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{}remove words with numbers python: https://stackoverflow.com/a/18082370/4084039}
         \PY{n}{sent\PYZus{}0} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S*}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{d}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sent\PYZus{}0}\PY{p}{)}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
A charming, rhyming book that describes the circumstances under which you eat (or don't) chicken soup with rice, month-by-month. This sounds like the kind of thing kids would make up while they're out of recess and sing over and over until they drive the teachers crazy. It's cute and catchy and sounds really childlike but is skillfully written.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{}remove spacial character: https://stackoverflow.com/a/5843547/4084039}
         \PY{n}{sent\PYZus{}1500} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}A\PYZhy{}Za\PYZhy{}z0\PYZhy{}9]+}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sent\PYZus{}1500}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}1500}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Strong without being bitter this is my favorite tea by far I was pleasantly surprised recently when i showed up for a production job and the craft services table had P G Tips Really helps on a cold early morning outside br br The shipper did exactly as promised 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} https://gist.github.com/sebleier/554280}
         \PY{c+c1}{\PYZsh{} we are removing the words from the stop words list: \PYZsq{}no\PYZsq{}, \PYZsq{}nor\PYZsq{}, \PYZsq{}not\PYZsq{}}
         \PY{c+c1}{\PYZsh{} \PYZlt{}br /\PYZgt{}\PYZlt{}br /\PYZgt{} ==\PYZgt{} after the above steps, we are getting \PYZdq{}br br\PYZdq{}}
         \PY{c+c1}{\PYZsh{} we are including them into stop words list}
         \PY{c+c1}{\PYZsh{} instead of \PYZlt{}br /\PYZgt{} if we have \PYZlt{}br/\PYZgt{} these tags would have revmoved in the 1st step}
         
         \PY{n}{stopwords}\PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{br}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{the}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{i}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{me}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{my}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{myself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{we}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{our}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ours}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ourselves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{you}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{you}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{re}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{you}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{you}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ll}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{you}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{d}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{your}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yours}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yourself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yourselves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{he}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{him}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{his}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{himself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{she}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{she}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{her}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{herself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{it}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{it}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{its}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{itself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{they}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{them}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{their}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{theirs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{themselves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{what}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{which}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{who}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whom}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{this}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{that}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{that}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ll}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{these}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{those}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{am}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{are}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{was}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{were}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{be}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{been}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{being}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{have}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{has}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{had}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{having}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{do}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{does}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{did}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{doing}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{an}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{the}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{and}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{but}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{if}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{or}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{because}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{as}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{until}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{while}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{of}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{by}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{for}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{with}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{about}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{against}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{between}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{into}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{through}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{during}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{before}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{after}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{above}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{below}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{to}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{from}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{up}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{down}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{in}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{out}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{on}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{over}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{under}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{again}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{further}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{then}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{once}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{here}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{there}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{when}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{where}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{why}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{how}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{all}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{any}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{both}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{each}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{few}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{more}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{most}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{other}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{some}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{such}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{own}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{so}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{than}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{too}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{very}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{t}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{can}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{will}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{just}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{don}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{don}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{should}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{should}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{ve}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{now}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{m}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{re}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ve}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ain}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{aren}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{aren}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{couldn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{couldn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{didn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{didn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{doesn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{doesn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hadn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hadn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hasn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hasn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{haven}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{haven}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{isn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{isn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mightn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mightn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mustn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PYZbs{}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mustn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{needn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{needn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{shan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shan}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{shouldn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{shouldn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wasn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wasn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weren}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{weren}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{won}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{won}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wouldn}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wouldn}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} Combining all the above stundents }
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
         \PY{n}{preprocessed\PYZus{}reviews} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} tqdm is for printing the status bar}
         \PY{k}{for} \PY{n}{sentance} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{:}
             \PY{n}{sentance} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sentance}\PY{p}{)}
             \PY{n}{sentance} \PY{o}{=} \PY{n}{BeautifulSoup}\PY{p}{(}\PY{n}{sentance}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lxml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{get\PYZus{}text}\PY{p}{(}\PY{p}{)}
             \PY{n}{sentance} \PY{o}{=} \PY{n}{decontracted}\PY{p}{(}\PY{n}{sentance}\PY{p}{)}
             \PY{n}{sentance} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S*}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{d}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sentance}\PY{p}{)}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}
             \PY{n}{sentance} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}A\PYZhy{}Za\PYZhy{}z]+}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sentance}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} https://gist.github.com/sebleier/554280}
             \PY{n}{sentance} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{sentance}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{e}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{stopwords}\PY{p}{)}
             \PY{n}{preprocessed\PYZus{}reviews}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sentance}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 71551/71551 [00:27<00:00, 2567.60it/s]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{preprocessed\PYZus{}reviews}\PY{p}{[}\PY{l+m+mi}{1500}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} 'strong without bitter favorite tea far pleasantly surprised recently showed production job craft services table p g tips really helps cold early morning outside shipper exactly promised'
\end{Verbatim}
            
    {[}3.2{]} Preprocessing Review Summary

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Similartly you can do preprocessing for review summary also.}
         \PY{n}{preprocessed\PYZus{}summaries} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} tqdm is for printing the status bar}
         \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Summary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{}remove URLs}
             \PY{n}{sentence} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{http}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sentence}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}remove hml tags }
             \PY{n}{sentence} \PY{o}{=} \PY{n}{BeautifulSoup}\PY{p}{(}\PY{n}{sentence}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lxml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{get\PYZus{}text}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} decontract : won\PYZsq{}t \PYZhy{}\PYZgt{} will not}
             \PY{n}{sentence} \PY{o}{=} \PY{n}{decontracted}\PY{p}{(}\PY{n}{sentence}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} remove words with numbers : eg abc123  or just 1234 are both filtered out}
             \PY{n}{sentence} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S*}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{d}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{S*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sentence}\PY{p}{)}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} remove special characters  }
             \PY{c+c1}{\PYZsh{} if we do not do the step above then this one will convert an \PYZsq{}abc123\PYZsq{} to an \PYZsq{}abc\PYZsq{} whereas}
             \PY{c+c1}{\PYZsh{} the above step will ensure that abc123 is completely removed from our result set.}
             \PY{n}{sentence} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[\PYZca{}A\PYZhy{}Za\PYZhy{}z]+}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{sentence}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} https://gist.github.com/sebleier/554280}
             \PY{c+c1}{\PYZsh{} also performing stemming here using Snowball stemmer.}
             \PY{c+c1}{\PYZsh{}if we stem then  pre\PYZhy{}trained Google W2V may fail to find a vector for tasti (the stemmed taste)}
             \PY{c+c1}{\PYZsh{}sentence = \PYZsq{} \PYZsq{}.join(sno.stem(e.lower()) for e in sentence.split() if e.lower() not in stopwords)}
             \PY{n}{sentence} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{e}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{stopwords}\PY{p}{)}
             \PY{n}{preprocessed\PYZus{}summaries}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 71551/71551 [00:17<00:00, 4007.48it/s]

    \end{Verbatim}

    \section{{[}4{]} Featurization}\label{featurization}

    \subsection{{[}4.1{]} BAG OF WORDS}\label{bag-of-words}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{}BoW}
        \PY{n}{count\PYZus{}vect} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}in scikit\PYZhy{}learn}
        \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{some feature names }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
        
        \PY{n}{final\PYZus{}counts} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the type of count vectorizer }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{final\PYZus{}counts}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the shape of out text BOW vectorizer }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{final\PYZus{}counts}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the number of unique words }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{final\PYZus{}counts}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
some feature names  ['aa', 'aahhhs', 'aback', 'abandon', 'abates', 'abbott', 'abby', 'abdominal', 'abiding', 'ability']
==================================================
the type of count vectorizer  <class 'scipy.sparse.csr.csr\_matrix'>
the shape of out text BOW vectorizer  (4986, 12997)
the number of unique words  12997

    \end{Verbatim}

    \subsection{{[}4.2{]} Bi-Grams and
n-Grams.}\label{bi-grams-and-n-grams.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{}bi\PYZhy{}gram, tri\PYZhy{}gram and n\PYZhy{}gram}
        
        \PY{c+c1}{\PYZsh{}removing stop words like \PYZdq{}not\PYZdq{} should be avoided before building n\PYZhy{}grams}
        \PY{c+c1}{\PYZsh{} count\PYZus{}vect = CountVectorizer(ngram\PYZus{}range=(1,2))}
        \PY{c+c1}{\PYZsh{} please do read the CountVectorizer documentation http://scikit\PYZhy{}learn.org/stable/modules/generated/sklearn.feature\PYZus{}extraction.text.CountVectorizer.html}
        
        \PY{c+c1}{\PYZsh{} you can choose these numebrs min\PYZus{}df=10, max\PYZus{}features=5000, of your choice}
        \PY{n}{count\PYZus{}vect} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{5000}\PY{p}{)}
        \PY{n}{final\PYZus{}bigram\PYZus{}counts} \PY{o}{=} \PY{n}{count\PYZus{}vect}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the type of count vectorizer }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{final\PYZus{}bigram\PYZus{}counts}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the shape of out text BOW vectorizer }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{final\PYZus{}bigram\PYZus{}counts}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the number of unique words including both unigrams and bigrams }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{final\PYZus{}bigram\PYZus{}counts}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
the type of count vectorizer  <class 'scipy.sparse.csr.csr\_matrix'>
the shape of out text BOW vectorizer  (4986, 3144)
the number of unique words including both unigrams and bigrams  3144

    \end{Verbatim}

    \subsection{{[}4.3{]} TF-IDF}\label{tf-idf}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{n}{tf\PYZus{}idf\PYZus{}vect} \PY{o}{=} \PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
        \PY{n}{tf\PYZus{}idf\PYZus{}vect}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{some sample features(unique words in the corpus)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{tf\PYZus{}idf\PYZus{}vect}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
        
        \PY{n}{final\PYZus{}tf\PYZus{}idf} \PY{o}{=} \PY{n}{tf\PYZus{}idf\PYZus{}vect}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the type of count vectorizer }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{type}\PY{p}{(}\PY{n}{final\PYZus{}tf\PYZus{}idf}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the shape of out text TFIDF vectorizer }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{final\PYZus{}tf\PYZus{}idf}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{the number of unique words including both unigrams and bigrams }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{final\PYZus{}tf\PYZus{}idf}\PY{o}{.}\PY{n}{get\PYZus{}shape}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
some sample features(unique words in the corpus) ['ability', 'able', 'able find', 'able get', 'absolute', 'absolutely', 'absolutely delicious', 'absolutely love', 'absolutely no', 'according']
==================================================
the type of count vectorizer  <class 'scipy.sparse.csr.csr\_matrix'>
the shape of out text TFIDF vectorizer  (4986, 3144)
the number of unique words including both unigrams and bigrams  3144

    \end{Verbatim}

    \subsection{{[}4.4{]} Word2Vec}\label{word2vec}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{} Train your own Word2Vec model using your own text corpus}
        \PY{n}{i}\PY{o}{=}\PY{l+m+mi}{0}
        \PY{n}{list\PYZus{}of\PYZus{}sentance}\PY{o}{=}\PY{p}{[}\PY{p}{]}
        \PY{k}{for} \PY{n}{sentance} \PY{o+ow}{in} \PY{n}{preprocessed\PYZus{}reviews}\PY{p}{:}
            \PY{n}{list\PYZus{}of\PYZus{}sentance}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sentance}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{} Using Google News Word2Vectors}
        
        \PY{c+c1}{\PYZsh{} in this project we are using a pretrained model by google}
        \PY{c+c1}{\PYZsh{} its 3.3G file, once you load this into your memory }
        \PY{c+c1}{\PYZsh{} it occupies \PYZti{}9Gb, so please do this step only if you have \PYZgt{}12G of ram}
        \PY{c+c1}{\PYZsh{} we will provide a pickle file wich contains a dict , }
        \PY{c+c1}{\PYZsh{} and it contains all our courpus words as keys and  model[word] as values}
        \PY{c+c1}{\PYZsh{} To use this code\PYZhy{}snippet, download \PYZdq{}GoogleNews\PYZhy{}vectors\PYZhy{}negative300.bin\PYZdq{} }
        \PY{c+c1}{\PYZsh{} from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit}
        \PY{c+c1}{\PYZsh{} it\PYZsq{}s 1.9GB in size.}
        
        
        \PY{c+c1}{\PYZsh{} http://kavita\PYZhy{}ganesan.com/gensim\PYZhy{}word2vec\PYZhy{}tutorial\PYZhy{}starter\PYZhy{}code/\PYZsh{}.W17SRFAzZPY}
        \PY{c+c1}{\PYZsh{} you can comment this whole cell}
        \PY{c+c1}{\PYZsh{} or change these varible according to your need}
        
        \PY{n}{is\PYZus{}your\PYZus{}ram\PYZus{}gt\PYZus{}16g}\PY{o}{=}\PY{k+kc}{False}
        \PY{n}{want\PYZus{}to\PYZus{}use\PYZus{}google\PYZus{}w2v} \PY{o}{=} \PY{k+kc}{False}
        \PY{n}{want\PYZus{}to\PYZus{}train\PYZus{}w2v} \PY{o}{=} \PY{k+kc}{True}
        
        \PY{k}{if} \PY{n}{want\PYZus{}to\PYZus{}train\PYZus{}w2v}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} min\PYZus{}count = 5 considers only words that occured atleast 5 times}
            \PY{n}{w2v\PYZus{}model}\PY{o}{=}\PY{n}{Word2Vec}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentance}\PY{p}{,}\PY{n}{min\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{workers}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{most\PYZus{}similar}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{great}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{=}\PY{l+s+s1}{\PYZsq{}}\PY{o}{*}\PY{l+m+mi}{50}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{most\PYZus{}similar}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{worst}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            
        \PY{k}{elif} \PY{n}{want\PYZus{}to\PYZus{}use\PYZus{}google\PYZus{}w2v} \PY{o+ow}{and} \PY{n}{is\PYZus{}your\PYZus{}ram\PYZus{}gt\PYZus{}16g}\PY{p}{:}
            \PY{k}{if} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{isfile}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GoogleNews\PYZhy{}vectors\PYZhy{}negative300.bin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
                \PY{n}{w2v\PYZus{}model}\PY{o}{=}\PY{n}{KeyedVectors}\PY{o}{.}\PY{n}{load\PYZus{}word2vec\PYZus{}format}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{GoogleNews\PYZhy{}vectors\PYZhy{}negative300.bin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{binary}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{most\PYZus{}similar}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{great}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{most\PYZus{}similar}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{worst}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{you don}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{t have gogole}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{s word2vec file, keep want\PYZus{}to\PYZus{}train\PYZus{}w2v = True, to train your own w2v }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('snack', 0.9951335191726685), ('calorie', 0.9946465492248535), ('wonderful', 0.9946032166481018), ('excellent', 0.9944332838058472), ('especially', 0.9941144585609436), ('baked', 0.9940600395202637), ('salted', 0.994047224521637), ('alternative', 0.9937226176261902), ('tasty', 0.9936816692352295), ('healthy', 0.9936649799346924)]
==================================================
[('varieties', 0.9994194507598877), ('become', 0.9992934465408325), ('popcorn', 0.9992750883102417), ('de', 0.9992610216140747), ('miss', 0.9992451071739197), ('melitta', 0.999218761920929), ('choice', 0.9992102384567261), ('american', 0.9991837739944458), ('beef', 0.9991780519485474), ('finish', 0.9991567134857178)]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{n}{w2v\PYZus{}words} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{vocab}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{number of words that occured minimum 5 times }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{w2v\PYZus{}words}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sample words }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{w2v\PYZus{}words}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{50}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
number of words that occured minimum 5 times  3817
sample words  ['product', 'available', 'course', 'total', 'pretty', 'stinky', 'right', 'nearby', 'used', 'ca', 'not', 'beat', 'great', 'received', 'shipment', 'could', 'hardly', 'wait', 'try', 'love', 'call', 'instead', 'removed', 'easily', 'daughter', 'designed', 'printed', 'use', 'car', 'windows', 'beautifully', 'shop', 'program', 'going', 'lot', 'fun', 'everywhere', 'like', 'tv', 'computer', 'really', 'good', 'idea', 'final', 'outstanding', 'window', 'everybody', 'asks', 'bought', 'made']

    \end{Verbatim}

    \subsection{{[}4.4.1{]} Converting text into vectors using Avg W2V,
TFIDF-W2V}\label{converting-text-into-vectors-using-avg-w2v-tfidf-w2v}

    \paragraph{{[}4.4.1.1{]} Avg W2v}\label{avg-w2v}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{} average Word2Vec}
        \PY{c+c1}{\PYZsh{} compute average word2vec for each review.}
        \PY{n}{sent\PYZus{}vectors} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{;} \PY{c+c1}{\PYZsh{} the avg\PYZhy{}w2v for each sentence/review is stored in this list}
        \PY{k}{for} \PY{n}{sent} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentance}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each review/sentence}
            \PY{n}{sent\PYZus{}vec} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)} \PY{c+c1}{\PYZsh{} as word vectors are of zero length 50, you might need to change this to 300 if you use google\PYZsq{}s w2v}
            \PY{n}{cnt\PYZus{}words} \PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;} \PY{c+c1}{\PYZsh{} num of words with a valid vector in the sentence/review}
            \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{sent}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each word in a review/sentence}
                \PY{k}{if} \PY{n}{word} \PY{o+ow}{in} \PY{n}{w2v\PYZus{}words}\PY{p}{:}
                    \PY{n}{vec} \PY{o}{=} \PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{p}{[}\PY{n}{word}\PY{p}{]}
                    \PY{n}{sent\PYZus{}vec} \PY{o}{+}\PY{o}{=} \PY{n}{vec}
                    \PY{n}{cnt\PYZus{}words} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{if} \PY{n}{cnt\PYZus{}words} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{sent\PYZus{}vec} \PY{o}{/}\PY{o}{=} \PY{n}{cnt\PYZus{}words}
            \PY{n}{sent\PYZus{}vectors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sent\PYZus{}vec}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sent\PYZus{}vectors}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sent\PYZus{}vectors}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 4986/4986 [00:03<00:00, 1330.47it/s]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
4986
50

    \end{Verbatim}

    \paragraph{{[}4.4.1.2{]} TFIDF weighted W2v}\label{tfidf-weighted-w2v}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{} S = [\PYZdq{}abc def pqr\PYZdq{}, \PYZdq{}def def def abc\PYZdq{}, \PYZdq{}pqr pqr def\PYZdq{}]}
        \PY{n}{model} \PY{o}{=} \PY{n}{TfidfVectorizer}\PY{p}{(}\PY{p}{)}
        \PY{n}{tf\PYZus{}idf\PYZus{}matrix} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} we are converting a dictionary with word as a key, and the idf as a value}
        \PY{n}{dictionary} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n+nb}{list}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{idf\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}0}]:} \PY{c+c1}{\PYZsh{} TF\PYZhy{}IDF weighted Word2Vec}
        \PY{n}{tfidf\PYZus{}feat} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} tfidf words/col\PYZhy{}names}
        \PY{c+c1}{\PYZsh{} final\PYZus{}tf\PYZus{}idf is the sparse matrix with row= sentence, col=word and cell\PYZus{}val = tfidf}
        
        \PY{n}{tfidf\PYZus{}sent\PYZus{}vectors} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{;} \PY{c+c1}{\PYZsh{} the tfidf\PYZhy{}w2v for each sentence/review is stored in this list}
        \PY{n}{row}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;}
        \PY{k}{for} \PY{n}{sent} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentance}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each review/sentence }
            \PY{n}{sent\PYZus{}vec} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)} \PY{c+c1}{\PYZsh{} as word vectors are of zero length}
            \PY{n}{weight\PYZus{}sum} \PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;} \PY{c+c1}{\PYZsh{} num of words with a valid vector in the sentence/review}
            \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{sent}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each word in a review/sentence}
                \PY{k}{if} \PY{n}{word} \PY{o+ow}{in} \PY{n}{w2v\PYZus{}words} \PY{o+ow}{and} \PY{n}{word} \PY{o+ow}{in} \PY{n}{tfidf\PYZus{}feat}\PY{p}{:}
                    \PY{n}{vec} \PY{o}{=} \PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{p}{[}\PY{n}{word}\PY{p}{]}
        \PY{c+c1}{\PYZsh{}             tf\PYZus{}idf = tf\PYZus{}idf\PYZus{}matrix[row, tfidf\PYZus{}feat.index(word)]}
                    \PY{c+c1}{\PYZsh{} to reduce the computation we are }
                    \PY{c+c1}{\PYZsh{} dictionary[word] = idf value of word in whole courpus}
                    \PY{c+c1}{\PYZsh{} sent.count(word) = tf valeus of word in this review}
                    \PY{n}{tf\PYZus{}idf} \PY{o}{=} \PY{n}{dictionary}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n}{sent}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{n}{word}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sent}\PY{p}{)}\PY{p}{)}
                    \PY{n}{sent\PYZus{}vec} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{vec} \PY{o}{*} \PY{n}{tf\PYZus{}idf}\PY{p}{)}
                    \PY{n}{weight\PYZus{}sum} \PY{o}{+}\PY{o}{=} \PY{n}{tf\PYZus{}idf}
            \PY{k}{if} \PY{n}{weight\PYZus{}sum} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{sent\PYZus{}vec} \PY{o}{/}\PY{o}{=} \PY{n}{weight\PYZus{}sum}
            \PY{n}{tfidf\PYZus{}sent\PYZus{}vectors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sent\PYZus{}vec}\PY{p}{)}
            \PY{n}{row} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 4986/4986 [00:20<00:00, 245.63it/s]

    \end{Verbatim}

    \section{{[}5{]} Assignment 5: Apply Logistic
Regression}\label{assignment-5-apply-logistic-regression}

    \begin{verbatim}
<li><strong>Apply Logistic Regression on these feature sets</strong>
    <ul>
        <li><font color='red'>SET 1:</font>Review text, preprocessed one converted into vectors using (BOW)</li>
        <li><font color='red'>SET 2:</font>Review text, preprocessed one converted into vectors using (TFIDF)</li>
        <li><font color='red'>SET 3:</font>Review text, preprocessed one converted into vectors using (AVG W2v)</li>
        <li><font color='red'>SET 4:</font>Review text, preprocessed one converted into vectors using (TFIDF W2v)</li>
    </ul>
</li>
<br>
<li><strong>Hyper paramter tuning (find best hyper parameters corresponding the algorithm that you choose)</strong>
    <ul>
<li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>
<li>Find the best hyper paramter using k-fold cross validation or simple cross validation data</li>
<li>Use gridsearch cv or randomsearch cv or you can also write your own for loops to do this task of hyperparameter tuning</li>          
    </ul>
</li>
<br>
<li><strong>Pertubation Test</strong>
    <ul>
<li>Get the weights W after fit your model with the data X i.e Train data.</li>
<li>Add a noise to the X (X' = X + e) and get the new data set X' (if X is a sparse
\end{verbatim}

matrix, X.data+=e)

\begin{verbatim}
<li>Fit the model again on data X' and get the weights W'</li>
<li>Add a small eps value(to eliminate the divisible by zero error) to W and W i.e
\end{verbatim}

W=W+10\^{}-6 and W' = W'+10\^{}-6

\begin{verbatim}
<li>Now find the % change between W and W' (| (W-W') / (W) |)*100)</li>
<li>Calculate the 0th, 10th, 20th, 30th, ...100th percentiles, and observe any sudden rise in the values of percentage_change_vector</li>
<li> Ex: consider your 99th percentile is 1.3 and your 100th percentiles are 34.6, there is sudden rise from 1.3 to 34.6, now calculate the 99.1, 99.2, 99.3,..., 100th percentile values and get the proper value after which there is sudden rise the values, assume it is 2.5</li>
        <li> Print the feature names whose % change is more than a threshold x(in our example it's 2.5)</li>
    </ul>
</li>
<br>
<li><strong>Sparsity</strong>
    <ul>
<li>Calculate sparsity on weight vector obtained after using L1 regularization</li>
    </ul>
</li>
<br><font color='red'>NOTE: Do sparsity and multicollinearity for any one of the vectorizers. Bow or tf-idf is recommended.</font>
<br>
<br>
<li><strong>Feature importance</strong>
    <ul>
<li>Get top 10 important features for both positive and negative classes separately.</li>
    </ul>
</li>
<br>
<li><strong>Feature engineering</strong>
    <ul>
<li>To increase the performance of your model, you can also experiment with with feature engineering like :</li>
        <ul>
        <li>Taking length of reviews as another feature.</li>
        <li>Considering some features from review summary as well.</li>
    </ul>
    </ul>
</li>
<br>
<li><strong>Representation of results</strong>
    <ul>
<li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure.
<img src='train_cv_auc.JPG' width=300px></li>
<li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.
<img src='train_test_auc.JPG' width=300px></li>
<li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points. Please visualize your confusion matrices using <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heatmaps.
<img src='confusion_matrix.png' width=300px></li>
    </ul>
</li>
<br>
<li><strong>Conclusion</strong>
    <ul>
<li>You need to summarize the results at the end of the notebook, summarize it in the table format. To print out a table please refer to this prettytable library<a href='http://zetcode.com/python/prettytable/'>  link</a> 
    <img src='summary.JPG' width=400px>
</li>
    </ul>
\end{verbatim}

    Note: Data Leakage

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There will be an issue of data-leakage if you vectorize the entire
  data and then split it into train/cv/test.
\item
  To avoid the issue of data-leakag, make sure to split your data first
  and then vectorize it.
\item
  While vectorizing your data, apply the method fit\_transform() on you
  train data, and apply the method transform() on cv/test data.
\item
  For more details please go through this link.
\end{enumerate}

    \section{Applying Logistic
Regression}\label{applying-logistic-regression}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{}concatenate the review summary with review text.}
         \PY{n}{preprocessed\PYZus{}text\PYZus{}n\PYZus{}summary} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+}  \PY{n}{preprocessed\PYZus{}summaries}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{preprocessed\PYZus{}reviews}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{preprocessed\PYZus{}text\PYZus{}n\PYZus{}summary}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{preprocessed\PYZus{}text\PYZus{}n\PYZus{}summary}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
charming rhyming book describes circumstances eat not chicken soup rice month month sounds like kind thing kids would make recess sing drive teachers crazy cute catchy sounds really childlike skillfully written charming childlike
71551

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LogisticRegression}
         
         \PY{c+c1}{\PYZsh{} https://scikit\PYZhy{}learn.org/stable/modules/generated/sklearn.metrics.roc\PYZus{}curve.html\PYZsh{}sklearn.metrics.roc\PYZus{}curve}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}curve}\PY{p}{,} \PY{n}{auc}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}auc\PYZus{}score}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{k+kn}{import} \PY{n+nn}{math}
         \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
         
         \PY{n}{C} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{4}\PY{p}{]}
         
         \PY{n}{Y} \PY{o}{=} \PY{n}{final}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} adding the summary text to get additional features}
         \PY{n}{X} \PY{o}{=} \PY{n}{preprocessed\PYZus{}text\PYZus{}n\PYZus{}summary}
         
         \PY{c+c1}{\PYZsh{} Train on oldest data (eg. Now \PYZhy{} 90 days), CV on somewhat recent  data (eg. Now \PYZhy{} 30 days) and Test on recent data (T\PYZhy{}15)}
         \PY{c+c1}{\PYZsh{} doing a time series split:  swapped the test and train as our data is in DESCENDING time order}
         \PY{c+c1}{\PYZsh{} and we want X\PYZus{}test to have the most recent data.}
         \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.77}\PY{p}{,} \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} trying a random shuffle to see if it performs better}
         \PY{c+c1}{\PYZsh{}X\PYZus{}train, X\PYZus{}test, y\PYZus{}train, y\PYZus{}test = train\PYZus{}test\PYZus{}split(X, Y, test\PYZus{}size=0.33, shuffle=True)}
         \PY{c+c1}{\PYZsh{} time series splitting}
         \PY{c+c1}{\PYZsh{} not splitting further into CV and Train because we plan to use GridSearch with 3 fold CV and so}
         \PY{c+c1}{\PYZsh{} we pass in entire X\PYZus{}train to GridSearch.}
         \PY{c+c1}{\PYZsh{}X\PYZus{}cv, X\PYZus{}train, y\PYZus{}cv, y\PYZus{}train = train\PYZus{}test\PYZus{}split(X\PYZus{}train, y\PYZus{}train, test\PYZus{}size=0.77, shuffle=False) }
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}train size=}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}test size=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y\PYZus{}train class counts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y\PYZus{}test class counts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
X\_train size= 55095
X\_test size= 16456
y\_train class counts
1    45830
0     9265
Name: Score, dtype: int64
y\_test class counts
1    13426
0     3030
Name: Score, dtype: int64

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{k+kn}{import} \PY{n+nn}{math}
         \PY{k+kn}{import} \PY{n+nn}{operator}
         
         \PY{k}{def} \PY{n+nf}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}\PY{p}{:}
             \PY{n}{train\PYZus{}fpr}\PY{p}{,} \PY{n}{train\PYZus{}tpr}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{test\PYZus{}fpr}\PY{p}{,} \PY{n}{test\PYZus{}tpr}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict\PYZus{}proba}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}fpr}\PY{p}{,} \PY{n}{train\PYZus{}tpr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train AUC =}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{auc}\PY{p}{(}\PY{n}{train\PYZus{}fpr}\PY{p}{,} \PY{n}{train\PYZus{}tpr}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}fpr}\PY{p}{,} \PY{n}{test\PYZus{}tpr}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{test AUC =}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{auc}\PY{p}{(}\PY{n}{test\PYZus{}fpr}\PY{p}{,} \PY{n}{test\PYZus{}tpr}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{fpr}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tpr}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ERROR PLOTS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=}\PY{l+s+s2}{\PYZdq{}}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
             
             \PY{n}{cmTrain} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}print(\PYZdq{}Test confusion matrix\PYZdq{})}
             \PY{n}{cmTest}\PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
         
           
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
             \PY{n}{trainx}\PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{cmTrain}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{trainx}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fmt}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{c+c1}{\PYZsh{}annot=True to annotate cells}
             \PY{c+c1}{\PYZsh{} labels, title and ticks}
             \PY{n}{trainx}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}\PY{n}{trainx}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} 
             \PY{n}{trainx}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train Confusion Matrix}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} 
             \PY{n}{trainx}\PY{o}{.}\PY{n}{xaxis}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{negative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{;} \PY{n}{trainx}\PY{o}{.}\PY{n}{yaxis}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{negative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{;}
             
             \PY{n}{testx}\PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
             \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{cmTest}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{testx}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blues}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fmt}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{g}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} \PY{c+c1}{\PYZsh{}annot=True to annotate cells}
             \PY{c+c1}{\PYZsh{} labels, title and ticks}
             \PY{n}{testx}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Actual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}\PY{n}{testx}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} 
             \PY{n}{testx}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Confusion Matrix}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;} 
             \PY{n}{testx}\PY{o}{.}\PY{n}{xaxis}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{negative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{;} \PY{n}{testx}\PY{o}{.}\PY{n}{yaxis}\PY{o}{.}\PY{n}{set\PYZus{}ticklabels}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{negative}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{positive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{;} 
         
         \PY{k}{def} \PY{n+nf}{sort\PYZus{}tuplelist}\PY{p}{(}\PY{n}{tupleList}\PY{p}{)}\PY{p}{:} 
             \PY{n}{tupleList}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{n}{key} \PY{o}{=} \PY{n}{operator}\PY{o}{.}\PY{n}{itemgetter}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{tupleList}  
         
         \PY{k}{def} \PY{n+nf}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{n}{reg}\PY{p}{,} \PY{n}{C}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}data}\PY{p}{)}\PY{p}{:}
             \PY{n}{parameters} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{C}\PY{p}{\PYZcb{}}\PY{p}{]}
             \PY{n}{clf} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{penalty}\PY{o}{=}\PY{n}{reg}\PY{p}{,}\PY{n}{class\PYZus{}weight}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{balanced}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{clf} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{clf}\PY{p}{,} \PY{n}{parameters}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{roc\PYZus{}auc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}data}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}data}\PY{p}{)}
         
             \PY{n}{train\PYZus{}auc}\PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}train\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{train\PYZus{}auc\PYZus{}std}\PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{std\PYZus{}train\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{cv\PYZus{}auc} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} 
             \PY{n}{cv\PYZus{}auc\PYZus{}std}\PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{std\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{C}\PY{p}{,} \PY{n}{train\PYZus{}auc}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} this code is copied from here: https://stackoverflow.com/a/48803361/4084039}
             \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{C}\PY{p}{,} \PY{n}{train\PYZus{}auc} \PY{o}{\PYZhy{}} \PY{n}{train\PYZus{}auc\PYZus{}std}\PY{p}{,}\PY{n}{train\PYZus{}auc} \PY{o}{+} \PY{n}{train\PYZus{}auc\PYZus{}std}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
             \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{C}\PY{p}{,} \PY{n}{cv\PYZus{}auc}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} this code is copied from here: https://stackoverflow.com/a/48803361/4084039}
             \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{C}\PY{p}{,} \PY{n}{cv\PYZus{}auc} \PY{o}{\PYZhy{}} \PY{n}{cv\PYZus{}auc\PYZus{}std}\PY{p}{,}\PY{n}{cv\PYZus{}auc} \PY{o}{+} \PY{n}{cv\PYZus{}auc\PYZus{}std}\PY{p}{,}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkorange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C: hyperparameter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ERROR PLOTS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{clf}
             
\end{Verbatim}


    \subsection{{[}5.1{]} Logistic Regression on BOW, SET
1}\label{logistic-regression-on-bow-set-1}

    \subsubsection{{[}5.1.1{]} Applying Logistic Regression with L1
regularization on BOW, SET
1}\label{applying-logistic-regression-with-l1-regularization-on-bow-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{CountVectorizer}
         \PY{k+kn}{import} \PY{n+nn}{numpy}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{confusion\PYZus{}matrix}
         
         
         \PY{n}{vectorizer} \PY{o}{=} \PY{n}{CountVectorizer}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} While vectorizing your data, apply the method fit\PYZus{}transform() on you train data, }
         \PY{c+c1}{\PYZsh{} and apply the method transform() on cv/test data.}
         \PY{c+c1}{\PYZsh{} THE VOCABULARY SHOULD BUILT ONLY WITH THE WORDS OF TRAIN DATA}
         \PY{n}{vectorizer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} we use the fitted CountVectorizer to convert the text to vector}
         \PY{n}{X\PYZus{}train\PYZus{}bow} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}bow} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{After vectorizations}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bow}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{p}{)}\PY{p}{)}
         
         
         \PY{n}{clf\PYZus{}l1} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf\PYZus{}l1}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bow}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf\PYZus{}l1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
After vectorizations
(55095, 45487) (55095,)
(16456, 45487) (16456,)
<class 'scipy.sparse.csr.csr\_matrix'>

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2570   460]
 [  927 12499]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{{[}5.1.1.1{]} Calculating sparsity on weight vector obtained
using L1 regularization on BOW, SET
1}\label{calculating-sparsity-on-weight-vector-obtained-using-l1-regularization-on-bow-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf\PYZus{}l1}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf\PYZus{}l1}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf\PYZus{}l1}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.9548704326711532
\{'C': 1\}
4585

    \end{Verbatim}

    \subsubsection{{[}5.1.2{]} Applying Logistic Regression with L2
regularization on BOW, SET
1}\label{applying-logistic-regression-with-l2-regularization-on-bow-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bow}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}bow}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2705   325]
 [ 1326 12100]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.957811157996479
\{'C': 0.01\}
45487

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{{[}5.1.2.1{]} Performing pertubation test (multicollinearity
check) on BOW, SET
1}\label{performing-pertubation-test-multicollinearity-check-on-bow-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{c+c1}{\PYZsh{}Get the weights W after fit your model with the data X i.e Train data.}
         \PY{c+c1}{\PYZsh{}Add a noise to the X (X\PYZsq{} = X + e) and get the new data set X\PYZsq{} (if X is a sparse matrix, X.data+=e)}
         \PY{c+c1}{\PYZsh{}Fit the model again on data X\PYZsq{} and get the weights W\PYZsq{}}
         \PY{c+c1}{\PYZsh{}Add a small eps value(to eliminate the divisible by zero error) to W and W i.e W=W+10\PYZca{}\PYZhy{}6 and W = W+10\PYZca{}\PYZhy{}6}
         \PY{c+c1}{\PYZsh{}Now find the \PYZpc{} change between W and W\PYZsq{} (| (W\PYZhy{}W\PYZsq{}) / (W) |)*100)}
         \PY{c+c1}{\PYZsh{}Calculate the 0th, 10th, 20th, 30th, ...100th percentiles, and observe any sudden rise in the values of percentage\PYZus{}change\PYZus{}vector}
         \PY{c+c1}{\PYZsh{}Ex: consider your 99th percentile is 1.3 and your 100th percentiles are 34.6, there is sudden rise from 1.3 to 34.6, now calculate the 99.1, 99.2, 99.3,..., 100th percentile values and get the proper value after which there is sudden rise the values, assume it is 2.5}
         \PY{c+c1}{\PYZsh{}Print the feature names whose \PYZpc{} change is more than a threshold x(in our example it\PYZsq{}s 2.5)}
         
         \PY{c+c1}{\PYZsh{} let\PYZsq{}s use L1 reg reults here}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf\PYZus{}l1}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n}{weights} \PY{o}{=} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{weights}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{weights}\PY{p}{)}
         \PY{n+nb}{len}\PY{p}{(}\PY{n}{weights}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}feature\PYZus{}names = vectorizer.get\PYZus{}feature\PYZus{}names()}
         \PY{c+c1}{\PYZsh{}dictionary = dict(zip(feature\PYZus{}names, weights))}
         \PY{c+c1}{\PYZsh{}print(dictionary)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}bow}\PY{p}{)}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}bow\PYZus{}perturbed} \PY{o}{=} \PY{n}{X\PYZus{}train\PYZus{}bow}\PY{o}{.}\PY{n}{asfptype}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} add noise to X\PYZus{}train\PYZus{}bow}
         \PY{n}{X\PYZus{}train\PYZus{}bow\PYZus{}perturbed}\PY{o}{.}\PY{n}{data} \PY{o}{+}\PY{o}{=} \PY{l+m+mf}{0.10}
         
         \PY{c+c1}{\PYZsh{}train again}
         \PY{n}{clf\PYZus{}l1} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}bow\PYZus{}perturbed}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}}
         \PY{n}{wp} \PY{o}{=} \PY{n}{clf\PYZus{}l1}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n}{w\PYZus{}perturbed} \PY{o}{=} \PY{n}{wp}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{weights} \PY{o}{=} \PY{n}{weights} \PY{o}{+} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{o}{\PYZhy{}}\PY{l+m+mi}{6}
         \PY{n}{w\PYZus{}perturbed} \PY{o}{=} \PY{n}{w\PYZus{}perturbed} \PY{o}{+} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{o}{\PYZhy{}}\PY{l+m+mi}{6}
         
         \PY{n}{percentage\PYZus{}change} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{absolute}\PY{p}{(}\PY{p}{(}\PY{n}{weights} \PY{o}{\PYZhy{}} \PY{n}{w\PYZus{}perturbed}\PY{p}{)}\PY{o}{/} \PY{n}{weights}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{100}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{)}\PY{p}{)}
             
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'numpy.ndarray'>
[0. 0. 0. {\ldots} 0. 0. 0.]
<class 'scipy.sparse.csr.csr\_matrix'>

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
[0. 0. 0. {\ldots} 0. 0. 0.]
10246
0
45487

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{c+c1}{\PYZsh{}https://stackoverflow.com/questions/26070514/how\PYZhy{}do\PYZhy{}i\PYZhy{}get\PYZhy{}the\PYZhy{}index\PYZhy{}of\PYZhy{}a\PYZhy{}specific\PYZhy{}percentile\PYZhy{}in\PYZhy{}numpy\PYZhy{}scipy}
         \PY{n}{percentile\PYZus{}array} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{percential\PYZus{}array\PYZus{}index} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{110}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
             \PY{n}{percentile\PYZus{}array}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{,} \PY{n}{a}\PY{p}{)}\PY{p}{)}
             \PY{n}{percential\PYZus{}array\PYZus{}index}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{,}\PY{n}{a}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{percentile\PYZus{}array}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{percential\PYZus{}array\PYZus{}index}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8377813657108503, 134019737.09577058]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 6454, 10246]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{c+c1}{\PYZsh{}the sudden change in value occurs between 90th and 100th percentile. }
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{90}\PY{p}{,}\PY{l+m+mi}{101}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{,} \PY{n}{a}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
0.8377813657108503
2.2526145081075324
3.570121334149716
4.904033269142897
6.310717205463914
7.929577817202095
10.404790832644618
15.966493985190866
30.76651415499005
99.04679598731391
134019737.09577058

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{c+c1}{\PYZsh{} the sudden change is actually between 99th percentile and 100th percentile}
         \PY{n}{z} \PY{o}{=} \PY{l+m+mf}{0.1}
         \PY{n}{p} \PY{o}{=} \PY{l+m+mf}{99.0}
         \PY{k}{for} \PY{n}{a} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{percentile=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{p}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{value=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{,} \PY{n}{p}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{index=}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{percentile}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{,}\PY{n}{p}\PY{p}{,}\PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{p} \PY{o}{+}\PY{o}{=} \PY{n}{z}
             
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
percentile= 99.0
value= 99.04679598731391
index= 31542
percentile= 99.1
value= 100.00392517700179
index= 25282
percentile= 99.19999999999999
value= 113.83395099829991
index= 5776
percentile= 99.29999999999998
value= 155.67658330113045
index= 17876
percentile= 99.39999999999998
value= 247.11166799681877
index= 24671
percentile= 99.49999999999997
value= 948.4522383198621
index= 34452
percentile= 99.59999999999997
value= 262679.03022406006
index= 4900
percentile= 99.69999999999996
value= 1762870.5483640572
index= 22812
percentile= 99.79999999999995
value= 4617951.6556952745
index= 34966
percentile= 99.89999999999995
value= 9810310.274979506
index= 32532
percentile= 99.99999999999994
value= 134019737.09516817
index= 10246

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{c+c1}{\PYZsh{} the sudden change is from value  948.452 to 262679.0302}
         \PY{c+c1}{\PYZsh{} feature names with value \PYZgt{} 948.452 are}
         \PY{n}{collinear\PYZus{}feature\PYZus{}indexes} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{v} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{percentage\PYZus{}change}\PY{p}{)} \PY{k}{if} \PY{n}{v} \PY{o}{\PYZgt{}} \PY{l+m+mf}{948.452}\PY{p}{]}
         \PY{n}{collinear\PYZus{}feature\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{n}{vectorizer}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{collinear\PYZus{}feature\PYZus{}indexes}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{collinear\PYZus{}feature\PYZus{}names}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{collinear\PYZus{}feature\PYZus{}names}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
['actual', 'advertising', 'agent', 'aggravating', 'al', 'amounts', 'andyou', 'aobut', 'aquire', 'asorbic', 'attributes', 'bang', 'barr', 'bear', 'behaves', 'bella', 'bigs', 'birds', 'bisquick', 'blehhhh', 'blunt', 'boasts', 'boldness', 'bounty', 'breville', 'brewers', 'brick', 'bullet', 'bushes', 'california', 'canadapedigree', 'carbonator', 'cares', 'celiac', 'central', 'chair', 'characteristics', 'characters', 'charmskeep', 'cheated', 'cheers', 'chewey', 'chipmunks', 'choc', 'chups', 'cigarettes', 'clarified', 'closes', 'coated', 'cocktail', 'combinations', 'confection', 'consideration', 'cookie', 'cornbread', 'cornelian', 'costly', 'crops', 'cross', 'db', 'dealt', 'declared', 'decomposes', 'deduct', 'defeats', 'defect', 'descriptions', 'detect', 'diabetic', 'differing', 'digested', 'directive', 'discounts', 'discs', 'doughnut', 'due', 'encounteredthis', 'established', 'exellent', 'exposed', 'fab', 'failure', 'fenugreek', 'flexible', 'floss', 'follow', 'forcing', 'frizz', 'frugal', 'gained', 'gardener', 'geez', 'gets', 'gofart', 'goodies', 'grand', 'grandson', 'grittiness', 'gulped', 'hem', 'herbed', 'hollow', 'honeydew', 'horton', 'hubert', 'hurry', 'icepack', 'ichibanya', 'ignorant', 'indistinguishable', 'infections', 'inhaling', 'inquire', 'instantto', 'insulation', 'internal', 'itchiness', 'iwould', 'jiffy', 'jimmies', 'kaffir', 'kinds', 'knock', 'knowing', 'knpw', 'lightweight', 'limits', 'lite', 'longest', 'lookout', 'loosing', 'lower', 'machines', 'marked', 'marrow', 'mask', 'math', 'miles', 'minimally', 'mixing', 'modest', 'naps', 'neatly', 'netrition', 'notasting', 'novus', 'nozzle', 'oats', 'occasional', 'oddly', 'odds', 'okc', 'omnivores', 'openings', 'osmosis', 'outrageous', 'overwhelmed', 'packages', 'pastry', 'patty', 'peanutbutter', 'peas', 'penetrate', 'penis', 'picle', 'pitch', 'pixkles', 'pizza', 'pockets', 'pots', 'presentation', 'pressed', 'propylene', 'punched', 'qualities', 'quicker', 'rao', 'recognition', 'reduced', 'reed', 'refridgeration', 'reminiscent', 'remove', 'removing', 'reviewers', 'said', 'salon', 'satisfy', 'scimpy', 'scooped', 'screwed', 'sealable', 'skim', 'sort', 'spaniels', 'spill', 'splitting', 'sprite', 'squashed', 'staple', 'stove', 'stumbled', 'suspected', 'sweet', 'system', 'tahini', 'tase', 'temporary', 'term', 'thoughts', 'tomato', 'tooooooo', 'traveled', 'tres', 'trigger', 'trust', 'uber', 'uninterested', 'unlucky', 'vanillas', 'verify', 'wel', 'welll', 'whipping', 'wildside', 'wished', 'workout', 'zinc']
228

    \end{Verbatim}

    \subsubsection{{[}5.1.3{]} Feature Importance on BOW, SET
1}\label{feature-importance-on-bow-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{}After you compute the weight vector, select the indexes of the highest 10 and lowest 10 coefficients}
         \PY{c+c1}{\PYZsh{}in the weight vector. The feature names for BOW, TF\PYZhy{}IDF can be obtained using get\PYZus{}features(). }
         \PY{c+c1}{\PYZsh{}Now from those features obtained, pick the features that are corresponding to the indexes of top 10 highest }
         \PY{c+c1}{\PYZsh{}and lowest coefficients.}
         
         \PY{c+c1}{\PYZsh{}Features associated with top 10 highest coefficients are the features that contribute more to the POSITIVE class.}
         \PY{c+c1}{\PYZsh{}Features associated with top 10 lowest coefficients are the features that contribute more to the NEGATIVE class.}
         \PY{c+c1}{\PYZsh{}using the results of L2 Regularized classfier.}
         \PY{n}{coefficients} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n}{w\PYZus{}star} \PY{o}{=} \PY{n}{coefficients}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}
         \PY{n}{tuples} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{,} \PY{n}{w\PYZus{}star}\PY{p}{)}\PY{p}{)}
         \PY{n}{sorted\PYZus{}tuples} \PY{o}{=} \PY{n}{sort\PYZus{}tuplelist}\PY{p}{(}\PY{n}{tuples}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}print(sorted\PYZus{}tuples)}
\end{Verbatim}


    \paragraph{{[}5.1.3.1{]} Top 10 important features of positive class
from SET
1}\label{top-10-important-features-of-positive-class-from-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{positive\PYZus{}features} \PY{o}{=} \PY{n}{sorted\PYZus{}tuples}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{:}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{positive\PYZus{}features}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('delicious', 1.004626028885019), ('perfect', 0.9025355326169747), ('great', 0.8936204383998242), ('best', 0.8775826633660397), ('excellent', 0.8542154341811756), ('wonderful', 0.7583027904845479), ('loves', 0.7551452856090028), ('favorite', 0.6875611011111497), ('love', 0.6671498719882305), ('nice', 0.650159654222829)]

    \end{Verbatim}

    \paragraph{{[}5.1.3.2{]} Top 10 important features of negative class
from SET
1}\label{top-10-important-features-of-negative-class-from-set-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{n}{negative\PYZus{}features} \PY{o}{=} \PY{n}{sorted\PYZus{}tuples}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{negative\PYZus{}features}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('horrible', -0.6540589934837376), ('awful', -0.6614138501535496), ('terrible', -0.70591149648597), ('money', -0.7123404993376071), ('stale', -0.7140545662660438), ('weak', -0.730862313900547), ('disappointing', -0.7316173087773233), ('unfortunately', -0.7803616616907552), ('worst', -0.788685481828633), ('disappointed', -1.0513273929838316)]

    \end{Verbatim}

    \subsection{{[}5.2{]} Logistic Regression on TFIDF, SET
2}\label{logistic-regression-on-tfidf-set-2}

    \subsubsection{{[}5.2.1{]} Applying Logistic Regression with L1
regularization on TFIDF, SET
2}\label{applying-logistic-regression-with-l1-regularization-on-tfidf-set-2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{TfidfTransformer}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{feature\PYZus{}extraction}\PY{n+nn}{.}\PY{n+nn}{text} \PY{k}{import} \PY{n}{TfidfVectorizer}
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
         
         
         \PY{n}{vectorizer} \PY{o}{=} \PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{vectorizer}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} we use the fitted vectorizer to convert the text to vector}
         \PY{n}{X\PYZus{}train\PYZus{}tfidf} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}tfidf} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{After vectorizations}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}tfidf}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}tfidf}\PY{p}{)}\PY{p}{)}
         
         
         
         \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
After vectorizations
(55095, 34572) (55095,)
(16456, 34572) (16456,)
<class 'scipy.sparse.csr.csr\_matrix'>

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2733   297]
 [ 1007 12419]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.9737804437378044
\{'C': 1\}
1533

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_79_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{{[}5.2.2{]} Applying Logistic Regression with L2
regularization on TFIDF, SET
2}\label{applying-logistic-regression-with-l2-regularization-on-tfidf-set-2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}tfidf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2743   287]
 [  909 12517]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.9767764531555196
\{'C': 1\}
34572

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_81_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{{[}5.2.3{]} Feature Importance on TFIDF, SET
2}\label{feature-importance-on-tfidf-set-2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{vectorizer}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}
         \PY{n}{coefficients} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n}{w\PYZus{}star} \PY{o}{=} \PY{n}{coefficients}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         
         \PY{n}{tuples} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{,} \PY{n}{w\PYZus{}star}\PY{p}{)}\PY{p}{)}
         \PY{n}{sorted\PYZus{}tuples} \PY{o}{=} \PY{n}{sort\PYZus{}tuplelist}\PY{p}{(}\PY{n}{tuples}\PY{p}{)}
\end{Verbatim}


    \paragraph{{[}5.2.3.1{]} Top 10 important features of positive class
from SET
2}\label{top-10-important-features-of-positive-class-from-set-2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{positive\PYZus{}features} \PY{o}{=} \PY{n}{sorted\PYZus{}tuples}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{10}\PY{p}{:}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{positive\PYZus{}features}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('wonderful', 6.503125165934264), ('favorite', 6.533597294901277), ('loves', 6.6960162448712035), ('perfect', 7.783323563940147), ('excellent', 8.755206271041406), ('love', 9.668454456761864), ('delicious', 9.817192220154066), ('good', 9.965414782666285), ('best', 10.70557929037365), ('great', 13.137823905682678)]

    \end{Verbatim}

    \paragraph{{[}5.2.3.2{]} Top 10 important features of negative class
from SET
2}\label{top-10-important-features-of-negative-class-from-set-2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{negative\PYZus{}features} \PY{o}{=} \PY{n}{sorted\PYZus{}tuples}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{negative\PYZus{}features}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[('not', -10.76939812252225), ('disappointed', -9.748632037483171), ('not good', -7.730860534189169), ('disappointing', -6.994293799908473), ('worst', -6.844886418984481), ('weak', -6.701595074554541), ('horrible', -6.665123312829005), ('terrible', -6.57890305686667), ('unfortunately', -6.143379197097252), ('not worth', -5.870891960457063)]

    \end{Verbatim}

    \subsection{{[}5.3{]} Logistic Regression on AVG W2V, SET
3}\label{logistic-regression-on-avg-w2v-set-3}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Word2Vec}
         \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{KeyedVectors}
         
         
         \PY{k}{def} \PY{n+nf}{computeAvgW2V}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
             \PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{o}{=}\PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{X}\PY{p}{:}
                 \PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} this line of code trains your w2v model on the give list of sentances}
             \PY{n}{w2v\PYZus{}model}\PY{o}{=}\PY{n}{Word2Vec}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{p}{,}\PY{n}{min\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{workers}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,}\PY{n+nb}{iter}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
             \PY{n}{w2v\PYZus{}words} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{vocab}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} average Word2Vec}
             \PY{c+c1}{\PYZsh{} compute average word2vec for each review.}
             \PY{n}{sent\PYZus{}vectors\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} the avg\PYZhy{}w2v for each sentence/review is stored in this list}
             \PY{k}{for} \PY{n}{sent} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each review/sentence}
                 \PY{n}{sent\PYZus{}vec} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)} \PY{c+c1}{\PYZsh{} as word vectors are of zero length 50, you might need to change this to 300 if you use google\PYZsq{}s w2v}
                 \PY{n}{cnt\PYZus{}words} \PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;} \PY{c+c1}{\PYZsh{} num of words with a valid vector in the sentence/review}
                 \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{sent}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each word in a review/sentence}
                     \PY{k}{if} \PY{n}{word} \PY{o+ow}{in} \PY{n}{w2v\PYZus{}words}\PY{p}{:}
                         \PY{n}{vec} \PY{o}{=} \PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{p}{[}\PY{n}{word}\PY{p}{]}
                         \PY{n}{sent\PYZus{}vec} \PY{o}{+}\PY{o}{=} \PY{n}{vec}
                         \PY{n}{cnt\PYZus{}words} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                 \PY{k}{if} \PY{n}{cnt\PYZus{}words} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n}{sent\PYZus{}vec} \PY{o}{/}\PY{o}{=} \PY{n}{cnt\PYZus{}words}
                 \PY{n}{sent\PYZus{}vectors\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sent\PYZus{}vec}\PY{p}{)}
             \PY{n}{sent\PYZus{}vectors\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{sent\PYZus{}vectors\PYZus{}train}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}vectors\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{sent\PYZus{}vectors\PYZus{}train}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
             \PY{k}{return} \PY{n}{sent\PYZus{}vectors\PYZus{}train}
\end{Verbatim}


    \subsubsection{{[}5.3.1{]} Applying Logistic Regression with L1
regularization on AVG W2V SET
3}\label{applying-logistic-regression-with-l1-regularization-on-avg-w2v-set-3}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{X\PYZus{}train\PYZus{}w2vAvg} \PY{o}{=} \PY{n}{computeAvgW2V}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}w2vAvg} \PY{o}{=} \PY{n}{computeAvgW2V}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         
         \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vAvg}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 55095/55095 [01:20<00:00, 681.46it/s] 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(55095, 50)
[-0.18921404 -0.041131   -0.31436349 -0.24266228  0.58941854  0.49746976
 -0.64779979 -0.04579963 -0.23828633 -0.30898747  0.27114164 -0.30109271
 -0.48232371  0.05522433 -0.09489444  0.33604911  0.45211268  0.0760485
  0.5149875   0.41913828  0.0344959   0.55790203  0.32099965  0.36027644
  0.41594385  0.24965249 -0.54011213 -0.06092552  0.15357676  0.02713726
  0.3246661   0.44625689 -0.62664956  0.85482717 -0.25234905  0.42406213
  0.88341015 -0.38683618  0.42223536  0.06676471 -0.26478488  0.41117351
 -0.568865   -0.56247315 -0.1730448   0.13604499 -0.18998017 -0.05540112
  0.59512974  0.20191685]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 16456/16456 [00:17<00:00, 935.47it/s] 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(16456, 50)
[ 0.60988894 -0.7913202   0.23412916 -0.11136134 -0.17425462 -0.11613596
  0.49990036 -0.08805562  0.31801226  0.09938592 -0.36080722 -0.05560918
  0.14462608 -0.08105637  0.68985326  0.21035072  0.14453829  0.46982845
  0.1211412  -0.01806533 -0.06238089  0.22374712 -0.14412294 -0.18814267
 -1.14709838  0.12458374 -0.49951372 -0.2119959  -0.6634607   0.38621513
 -0.18790756 -0.30233719  0.10153362 -0.33494587  0.18856375  0.5230891
  0.56783575 -0.21861941  0.56829925 -0.51153512 -0.18802216 -0.25038399
 -0.26520994 -0.79293803  0.14538662 -0.33775963 -0.12152057 -0.35491055
 -0.08031984  0.2104208 ]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2217   813]
 [ 2247 11179]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_6.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.8727607730235261
\{'C': 1\}
50

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_91_8.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{{[}5.3.2{]} Applying Logistic Regression with L2
regularization on AVG W2V, SET
3}\label{applying-logistic-regression-with-l2-regularization-on-avg-w2v-set-3}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vAvg}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vAvg}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2193   837]
 [ 2200 11226]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.8713369311011244
\{'C': 1\}
50

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{{[}5.4{]} Logistic Regression on TFIDF W2V, SET
4}\label{logistic-regression-on-tfidf-w2v-set-4}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{k}{def} \PY{n+nf}{computeTfIdfW2v}\PY{p}{(}\PY{n}{data}\PY{p}{)}\PY{p}{:}
             
             \PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{o}{=}\PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{sentence} \PY{o+ow}{in} \PY{n}{data}\PY{p}{:}
                 \PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sentence}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                 
             \PY{n}{model} \PY{o}{=} \PY{n}{TfidfVectorizer}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} we are converting a dictionary with word as a key, and the idf as a value}
             \PY{n}{dictionary} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n+nb}{list}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{idf\PYZus{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             
              \PY{c+c1}{\PYZsh{} this line of code trains your w2v model on the give list of sentances}
             \PY{n}{w2v\PYZus{}model}\PY{o}{=}\PY{n}{Word2Vec}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{p}{,}\PY{n}{min\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}\PY{n}{size}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{workers}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n+nb}{iter}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{)}
             \PY{n}{w2v\PYZus{}words} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{o}{.}\PY{n}{vocab}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} TF\PYZhy{}IDF weighted Word2Vec}
             \PY{n}{tfidf\PYZus{}feat} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{} tfidf words/col\PYZhy{}names}
             \PY{c+c1}{\PYZsh{} final\PYZus{}tf\PYZus{}idf is the sparse matrix with row= sentence, col=word and cell\PYZus{}val = tfidf}
         
             \PY{n}{tfidf\PYZus{}sent\PYZus{}vectors} \PY{o}{=} \PY{p}{[}\PY{p}{]}\PY{p}{;} \PY{c+c1}{\PYZsh{} the tfidf\PYZhy{}w2v for each sentence/review is stored in this list}
             \PY{n}{row}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;}
             \PY{k}{for} \PY{n}{sent} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}sentence\PYZus{}train}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each review/sentence }
                 \PY{n}{sent\PYZus{}vec} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)} \PY{c+c1}{\PYZsh{} as word vectors are of zero length}
                 \PY{n}{weight\PYZus{}sum} \PY{o}{=}\PY{l+m+mi}{0}\PY{p}{;} \PY{c+c1}{\PYZsh{} num of words with a valid vector in the sentence/review}
                 \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{sent}\PY{p}{:} \PY{c+c1}{\PYZsh{} for each word in a review/sentence}
                     \PY{k}{if} \PY{n}{word} \PY{o+ow}{in} \PY{n}{w2v\PYZus{}words} \PY{o+ow}{and} \PY{n}{word} \PY{o+ow}{in} \PY{n}{tfidf\PYZus{}feat}\PY{p}{:}
                         \PY{n}{vec} \PY{o}{=} \PY{n}{w2v\PYZus{}model}\PY{o}{.}\PY{n}{wv}\PY{p}{[}\PY{n}{word}\PY{p}{]}
                         \PY{c+c1}{\PYZsh{}tf\PYZus{}idf = tf\PYZus{}idf\PYZus{}matrix[row, tfidf\PYZus{}feat.index(word)]}
                         \PY{c+c1}{\PYZsh{} to reduce the computation we are }
                         \PY{c+c1}{\PYZsh{} dictionary[word] = idf value of word in whole courpus}
                         \PY{c+c1}{\PYZsh{} sent.count(word) = tf valeus of word in this review}
                         \PY{n}{tf\PYZus{}idf} \PY{o}{=} \PY{n}{dictionary}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n}{sent}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{n}{word}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sent}\PY{p}{)}\PY{p}{)}
                         \PY{n}{sent\PYZus{}vec} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{vec} \PY{o}{*} \PY{n}{tf\PYZus{}idf}\PY{p}{)}
                         \PY{n}{weight\PYZus{}sum} \PY{o}{+}\PY{o}{=} \PY{n}{tf\PYZus{}idf}
                 \PY{k}{if} \PY{n}{weight\PYZus{}sum} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n}{sent\PYZus{}vec} \PY{o}{/}\PY{o}{=} \PY{n}{weight\PYZus{}sum}
                 \PY{n}{tfidf\PYZus{}sent\PYZus{}vectors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{sent\PYZus{}vec}\PY{p}{)}
                 \PY{n}{row} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             \PY{k}{return} \PY{n}{tfidf\PYZus{}sent\PYZus{}vectors}
\end{Verbatim}


    \subsubsection{{[}5.4.1{]} Applying Logistic Regression with L1
regularization on TFIDF W2V, SET
4}\label{applying-logistic-regression-with-l1-regularization-on-tfidf-w2v-set-4}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{X\PYZus{}train\PYZus{}w2vTfIdf} \PY{o}{=} \PY{n}{computeTfIdfW2v}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf} \PY{o}{=} \PY{n}{computeTfIdfW2v}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|| 55095/55095 [32:35<00:00, 28.17it/s]  
100\%|| 16456/16456 [01:28<00:00, 185.28it/s]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_97_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 1732  1298]
 [ 2426 11000]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_97_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.7686414813088638
\{'C': 1\}
49

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_97_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{{[}5.4.2{]} Applying Logistic Regression with L2
regularization on TFIDF W2V, SET
4}\label{applying-logistic-regression-with-l2-regularization-on-tfidf-w2v-set-4}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{c+c1}{\PYZsh{} Please write all the code with proper documentation}
         \PY{n}{clf} \PY{o}{=} \PY{n}{train\PYZus{}and\PYZus{}plot\PYZus{}auc}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{C}\PY{p}{,}\PY{n}{X\PYZus{}train\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confusion Matrix : }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{predictAndPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{clf}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{clf}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}w2vTfIdf}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
         \PY{n}{bestC} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{bestC}\PY{p}{)}
         \PY{n}{w} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{coef\PYZus{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Confusion Matrix : 
[[ 2013  1017]
 [ 2544 10882]]

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
====================================================================================================
0.8121538230092933
\{'C': 0.01\}
50

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{{[}6{]} Conclusions}\label{conclusions}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{k+kn}{from} \PY{n+nn}{prettytable} \PY{k}{import} \PY{n}{PrettyTable}
         
         \PY{n}{x} \PY{o}{=} \PY{n}{PrettyTable}\PY{p}{(}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{field\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Vectorizer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Algorithm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Reg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{HyperParameter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DataSize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{min\PYZus{}count/min\PYZus{}df}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BOW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.931}\PY{p}{,} \PY{l+m+mf}{0.991}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BOW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.935}\PY{p}{,} \PY{l+m+mf}{0.957}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.960}\PY{p}{,} \PY{l+m+mf}{0.978}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.964}\PY{p}{,} \PY{l+m+mf}{0.987}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AVGW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.853}\PY{p}{,} \PY{l+m+mf}{0.920}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AVGW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.851}\PY{p}{,} \PY{l+m+mf}{0.920}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDFW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.833}\PY{p}{,}\PY{l+m+mf}{0.899}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDFW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.831}\PY{p}{,} \PY{l+m+mf}{0.899}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tabular Results: with TimeSeries Split}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]


Tabular Results: with TimeSeries Split
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+
| Vectorizer | Algorithm | Reg | HyperParameter | Test AUC | Train AUC | DataSize | min\_count/min\_df |
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+
|    BOW     |     LR    |  l1 |       1        |  0.931   |   0.991   |   100k   |        1         |
|    BOW     |     LR    |  l2 |      0.01      |  0.935   |   0.957   |   100k   |        1         |
|   TFIDF    |     LR    |  l1 |       1        |   0.96   |   0.978   |   100k   |        10        |
|   TFIDF    |     LR    |  l2 |       1        |  0.964   |   0.987   |   100k   |        10        |
|   AVGW2V   |     LR    |  l1 |       1        |  0.853   |    0.92   |   100k   |        20        |
|   AVGW2V   |     LR    |  l2 |       1        |  0.851   |    0.92   |   100k   |        20        |
|  TFIDFW2V  |     LR    |  l1 |       1        |  0.833   |   0.899   |   100k   |        20        |
|  TFIDFW2V  |     LR    |  l2 |       1        |  0.831   |   0.899   |   100k   |        20        |
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{k+kn}{from} \PY{n+nn}{prettytable} \PY{k}{import} \PY{n}{PrettyTable}
         
         \PY{n}{x} \PY{o}{=} \PY{n}{PrettyTable}\PY{p}{(}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{field\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Vectorizer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Algorithm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Reg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{HyperParameter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DataSize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{min\PYZus{}count/min\PYZus{}df}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BOW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.932}\PY{p}{,} \PY{l+m+mf}{0.991}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BOW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.937}\PY{p}{,} \PY{l+m+mf}{0.956}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.960}\PY{p}{,} \PY{l+m+mf}{0.977}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.964}\PY{p}{,} \PY{l+m+mf}{0.987}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AVGW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.867}\PY{p}{,} \PY{l+m+mf}{0.918}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AVGW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mf}{0.866}\PY{p}{,} \PY{l+m+mf}{0.918}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDFW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.793}\PY{p}{,} \PY{l+m+mf}{0.897}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDFW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.793}\PY{p}{,} \PY{l+m+mf}{0.897}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tabular Results: with Randomized Split (Shuffle=True)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]


Tabular Results: with Randomized Split (Shuffle=True)
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+
| Vectorizer | Algorithm | Reg | HyperParameter | Test AUC | Train AUC | DataSize | min\_count/min\_df |
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+
|    BOW     |     LR    |  l1 |       1        |  0.932   |   0.991   |   100k   |        1         |
|    BOW     |     LR    |  l2 |      0.01      |  0.937   |   0.956   |   100k   |        1         |
|   TFIDF    |     LR    |  l1 |       1        |   0.96   |   0.977   |   100k   |        10        |
|   TFIDF    |     LR    |  l2 |       1        |  0.964   |   0.987   |   100k   |        10        |
|   AVGW2V   |     LR    |  l1 |       1        |  0.867   |   0.918   |   100k   |        20        |
|   AVGW2V   |     LR    |  l2 |      100       |  0.866   |   0.918   |   100k   |        20        |
|  TFIDFW2V  |     LR    |  l1 |       1        |  0.793   |   0.897   |   100k   |        20        |
|  TFIDFW2V  |     LR    |  l2 |       1        |  0.793   |   0.897   |   100k   |        20        |
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{k+kn}{from} \PY{n+nn}{prettytable} \PY{k}{import} \PY{n}{PrettyTable}
         
         \PY{n}{x} \PY{o}{=} \PY{n}{PrettyTable}\PY{p}{(}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{field\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Vectorizer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Algorithm}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Reg}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{HyperParameter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train AUC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DataSize}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{min\PYZus{}count/min\PYZus{}df}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BOW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.954}\PY{p}{,} \PY{l+m+mf}{0.995}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{BOW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.957}\PY{p}{,} \PY{l+m+mf}{0.972}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.973}\PY{p}{,} \PY{l+m+mf}{0.986}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDF}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.976}\PY{p}{,} \PY{l+m+mf}{0.992}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AVGW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.872}\PY{p}{,} \PY{l+m+mf}{0.946}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AVGW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mf}{0.871}\PY{p}{,} \PY{l+m+mf}{0.946}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDFW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.768}\PY{p}{,} \PY{l+m+mf}{0.924}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n}{x}\PY{o}{.}\PY{n}{add\PYZus{}row}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TFIDFW2V}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.812}\PY{p}{,} \PY{l+m+mf}{0.924}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{100k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tabular Results: TimeSeries Split and ReviewText + ReviewSummary as Features}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]






Tabular Results: TimeSeries Split and ReviewText + ReviewSummary as Features
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+
| Vectorizer | Algorithm | Reg | HyperParameter | Test AUC | Train AUC | DataSize | min\_count/min\_df |
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+
|    BOW     |     LR    |  l1 |       1        |  0.954   |   0.995   |   100k   |        1         |
|    BOW     |     LR    |  l2 |      0.01      |  0.957   |   0.972   |   100k   |        1         |
|   TFIDF    |     LR    |  l1 |       1        |  0.973   |   0.986   |   100k   |        10        |
|   TFIDF    |     LR    |  l2 |       1        |  0.976   |   0.992   |   100k   |        10        |
|   AVGW2V   |     LR    |  l1 |       1        |  0.872   |   0.946   |   100k   |        20        |
|   AVGW2V   |     LR    |  l2 |      100       |  0.871   |   0.946   |   100k   |        20        |
|  TFIDFW2V  |     LR    |  l1 |       1        |  0.768   |   0.924   |   100k   |        20        |
|  TFIDFW2V  |     LR    |  l2 |       1        |  0.812   |   0.924   |   100k   |        20        |
+------------+-----------+-----+----------------+----------+-----------+----------+------------------+

    \end{Verbatim}

    \section{Observations}\label{observations}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  With BOW and TFIDF featurization Logistic Regression performs slightly
  better than Naive Bayes Classifier.
\item
  The Best score observed is for TFIDF with L1 or L2 regularization.
\item
  The Top 10 +ve and -ve feature importances with BOW and TFIDF
  featurization look much more sensible with Logistic Regression than
  with NaiveBayes.
\item
  For Both AVG and TFIDF W2V featurization the CV performance seemed to
  closely follow the Train Performance.

  4.1 However the Train performance was found to be lower than in case
  of BOW and TFIDF.\\
  4.2 And the difference between Train and Test AUC in case of W2V
  models is also slightly larger then the corresponding difference in
  case of BOW and TFIDF.
\item
  Tried to experiment with the W2V featurization by changing values for
  min\_count and iter. Tried values of 1, 5, 10, 20, 200 for min\_count
  and values 1, 10, 15 for iter. And i have reported the values for the
  best combination found. The reported values are slightly better than
  then defaults min\_count=5 and iter=1. But i was unable to get the
  performance closer to BOW/TFIDF.
\item
  Time series split and randomized split shows mixed results but overall
  the performance is approximately the same.
\item
  Improving the Feature Vector by adding Review Summary to Review Text
  clearly shows improved performance in both Test and Train, for all the
  featurizations.
\end{enumerate}

    Reasons for Poor Performance of W2V over Plain BOW/TFIDF

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  On the web i found one reference that says if Context is Domain
  specific then W2V may perform poorer than plain BOW/TFIDF. However for
  AFR Dataset i am not sure if context is the issue.
\item
  TODO : Still exploring other reasons.
\end{enumerate}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
